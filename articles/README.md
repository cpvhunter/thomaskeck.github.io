# Interesting articles on Machine Learning

An (incomplete) list of articles on Machine Learning that I read, mainly used as a reference I can access when I'm not at home


## On Deep Learning

Deep Learning is the current revolution ongoing in the field of machine learning. Everything from self-driving cars, speech recognition and playing Go can be accomplished using Deep Learning. There is a lot of research going on in HEP, howto take advantage of Deep Learning in our analysis. 
  
  - 24.06.2012 [Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. „Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives“.](https://arxiv.org/abs/1206.5538)
  - 03.07.2012 [G. E. Hinton „Improving neural networks by preventing co-adaptation of feature detectors“](https://arxiv.org/abs/1207.0580)
  - 2013 [L. Wan et. al. „Regularization of Neural Networks using DropConnect“](http://proceedings.mlr.press/v28/wan13.html)
  - 18.02.2013 [I. J. Goodfellow „Maxout Networks“](https://arxiv.org/abs/1302.4389)
  - 19.02.2014 [Pierre Baldi, Peter Sadowski, and Daniel Whiteson. „Searching for Exotic Particles in High-Energy Physics with Deep Learning“](https://arxiv.org/abs/1402.4735)
  - 28.05.2015 [Yann Lecun, Yoshua Bengio, and Geoffrey Hinton. „Deep learning“.](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)
  - 05.06.2017 [A. Santoro, et. al „A simple neural network module for relational reasoning“](https://arxiv.org/abs/1706.01427)
  - 24.06.2016 [H. Cheng, et. al. „Wide & Deep Learning for Recommender Systems“](https://arxiv.org/abs/1606.07792)
  - 29.08.2016 [Henry W. Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?](https://arxiv.org/abs/1608.08225)
  - 28.10.2017 [A. v.d. Oord, et. al „Parallel WaveNet: Fast High-Fidelity Speech Synthesis“](https://arxiv.org/abs/1711.10433)
  - 25.10.2017 [D. Levy, et. al. „Generalizing Hamiltonian Monte Carlo with Neural Networks“](https://arxiv.org/abs/1711.09268)
  - 15.02.2018 [A. S. Morcos et. al. „On the importance of single directions for generalization“](https://openreview.net/forum?id=r1iuQjxCZ&noteId=r1On1W5xf)
  - 22.02.2018 [N. Carlini, et. al. „The Secret Sharer: Measuring Unintended Neural Network Memorization & Extracting Secrets“](https://arxiv.org/abs/1802.08232)
  - 11.02.2015 [S. Ioffe, C. Szegedy „Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift“](https://arxiv.org/abs/1502.03167)


### Gradient Descent Optimization
  - 22.10.2014 [D. P. Kingma, J. Ba „Adam: A Method for Stochastic Optimization“](https://arxiv.org/abs/1412.6980)
  - 15.09.2016 [N. S. Keskar et. al. „On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima“](https://arxiv.org/abs/1609.04836)
  - 10.11.2016 [C. Zhang et. al. „Understanding deep learning requires rethinking generalization“](https://arxiv.org/abs/1611.03530)
  - 15.12.2016 [S. Ruder „An overview of gradient descent optimization algorithms“](https://arxiv.org/abs/1609.04747)
  - 19.02.2017 [D. Soudry et. al. „Exponentially vanishing sub-optimal local minima in multilayer neural networks“](https://arxiv.org/abs/1702.05777)
  - 15.03.2017 [L. Dinh et. al. „Sharp Minima Can Generalize For Deep Nets“](https://arxiv.org/abs/1703.04933)
  - 24.05.2017 [E. Hoffer et. al. „Train longer, generalize better: closing the generalization gap in large batch training of neural networks“](https://arxiv.org/abs/1705.08741)
  - 01.10.2017 [S. L. Smith et. al. „Don't Decay the Learning Rate, Increase the Batch Size“](https://arxiv.org/abs/1711.00489)
  - 14.10.2017 [I. Loshchilov, F. Hutter. „Fixing Weight Decay Regularization in Adam“](https://arxiv.org/abs/1711.05101)
  - 27.10.2017 [D. Soudry et. al. „The Implicit Bias of Gradient Descent on Separable Data“](https://arxiv.org/abs/1710.10345)
  - 05.12.2017 [Y. Lin et. al. „Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training“](https://arxiv.org/abs/1712.01887)

### Reinforcement Learning

  - 19.12.2013 [V. Minh, et. al „Playing Atari with Deep Reinforcement Learning“](https://arxiv.org/abs/1312.5602)
  - 27.01.2016 [D. Silver, et. al. „Mastering the game of Go with deep neural networks and tree search“](https://www.nature.com/articles/nature16961)
  - 04.02.2016 [V. Minh et al. „Asynchronous Methods for Deep Reinforcement Learning“](https://arxiv.org/abs/1602.01783)
  - 02.12.2016 [J. Kirkpatrick et. al. „Overcoming catastrophic forgetting in neural networks“](https://arxiv.org/abs/1612.00796)
  - 25.01.2017 [Y. Li „Deep Reinforcement Learning: An Overview“](https://arxiv.org/abs/1701.07274)
  - 28.02.2017 [O. Nachum et al. „Bridging the Gap Between Value and Policy Based Reinforcement Learning“](https://arxiv.org/abs/1702.08892)
  - 06.10.2017 [M. Hessel et. al. „Rainbow: Combining Improvements in Deep Reinforcement Learning“](https://arxiv.org/abs/1710.02298v1)
  - 18.10.2017 [D. Silver et al. „Mastering the game of Go without Human Knowledge“] (https://deepmind.com/research/publications/mastering-game-go-without-human-knowledge/)
  - 02.11.2017 [M. Lanctot et. al „A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning“](https://arxiv.org/abs/1711.00832)
  - 02.01.2018 [Y. Tassa, et. al. „DeepMind Control Suite“](https://arxiv.org/abs/1801.00690)
  - 15.02.2018 [G. Barth-Maron et al. „Distributed Distributional Deterministic Policy Gradients“](https://openreview.net/forum?id=SyZipzbCb)
  - 22.02.2018 [D. J. Mankowitz, et al. „Unicorn: Continual Learning with a Universal, Off-policy Agent“](https://arxiv.org/abs/1802.08294)
  - 24.02.2018 [P. Chrabaszcz, I. Loshchilov, F. Hutter „Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari“](https://arxiv.org/abs/1802.08842)

### Recurrent Neural Networks
  - 11.12.2014 [J. Chung et al. „Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling“](https://arxiv.org/abs/1412.3555)
  - 26.10.2017 [F. Hill et. al. „Understanding Grounded Language Learning Agents“](https://arxiv.org/abs/1710.09867)


### Convolutional Neural Networks
  - 06.12.2012 [A. Krizhevsky „ImageNet classification with deep convolutional neural networks“](https://cacm.acm.org/magazines/2017/6/217745-imagenet-classification-with-deep-convolutional-neural-networks/fulltext)
  - 23.04.2014 [A. Krizhevsky „One weird trick for parallelizing convolutional neural networks“](https://arxiv.org/abs/1404.5997)
  - 06.02.2015 [K. He et. al. „Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification“](https://arxiv.org/abs/1502.01852)
  - 17.11.2015 [O. Vinyals, et. al „Show and Tell: A Neural Image Caption Generator“](https://arxiv.org/abs/1411.4555)
  - 10.12.2015 [K. He et. al. „Deep Residual Learning for Image Recognition“](https://arxiv.org/abs/1512.03385)
  - 23.02.2016 [C. Szegedy et. al. „Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning“](https://arxiv.org/abs/1602.07261)
  - 21.05.2017 [X. Gastaldi „Shake-Shake regularization“](https://arxiv.org/abs/1705.07485)
  - 30.10.2017 [M. Babaeizadeh, et. al. „Stochastic Variational Video Prediction“](https://arxiv.org/abs/1710.11252)


### Adversarial Examples

  - 20.12.2014 [I. J. Goodfellow „Explaining and Harnessing Adversarial Examples“](https://arxiv.org/abs/1412.6572)
  - 28.03.2017 [S. Baluja, I. Fischer „Adversarial Transformation Networks: Learning to Generate Adversarial Examples“](https://arxiv.org/abs/1703.09387)
  - 09.01.2018 [J. Gilmer et. al. „Adversarial Spheres“](https://arxiv.org/abs/1801.02774)


### Adversarial Networks

  - 10.06.2014 [I. J. Goodfellow et. al „Generative Adversarial Networks“](https://arxiv.org/abs/1406.2661)
  - 19.10.2015 [A. Radford, et. al „Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks“](https://arxiv.org/abs/1511.06434)
  - 03.11.2016 [Gilles Louppe, Michael Kagan, and Kyle Cranmer. „Learning to Pivot with Adversarial Networks“.](https://arxiv.org/abs/1611.01046)


### Hyper Parameter Optimization
All multivariate methods have hyper-parameters, so some parameters which influence the performance of the algorithm and have to be set by the user. It is common to automatically optimize these hyper-parmaeters using different optimization algorithms. There are four different approaches: grid-search, random-search, gradient, bayesian

  - 13.02.2012 [James Bergstra and Yoshua Bengio. „Random Search for Hyper-parameter Optimization“](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)
  - 06.12.2012 [Jasper Snoek, Hugo Larochelle, and Ryan P Adams. „Practical Bayesian Optimization of Machine Learning Algorithms“.](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)
  - 11.02.2015 [Dougal Maclaurin, David Duvenaud, and Ryan Adams. „Gradient-based Hyperparameter Optimization through Reversible Learning“.](http://jmlr.org/proceedings/papers/v37/maclaurin15.pdf)
  - 01.12.2017 [H. Liu et. al „Hierarchical Representations for Efficient Architecture Search“](https://arxiv.org/abs/1711.00436)


## On Boosted Decision Trees

Boosted decision trees are the working horse of classification / regression in HEP. They have a good out-of-the-box performance, are reasonable fast, and robust

  - 24.02.1999 [Jerome H. Friedman. „Greedy Function Approximation: A Gradient Boosting Machine“](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
  - 26.03.1999 [Jerome H. Friedman. „Stochastic gradient boosting“](http://statweb.stanford.edu/~jhf/ftp/stobst.pdf)
  - 30.05.2013 [Justin Stevens, Mike Williams "uBoost: A boosting method for producing uniform selection efficiencies from multivariate classifiers"](https://arxiv.org/abs/1305.7248)
  - 30.03.2015 [Alex Rogozhnikov et al. „New approaches for boosting to uniformity“.](http://iopscience.iop.org/article/10.1088/1748-0221/10/03/T03002/meta)


## On Data Analysis Techniques

With sPlot you can train a classifier directly on data, other similar methods are: side-band substration and training data vs mc, both are described in the second paper below

  - 17.02.2004 [Muriel Pivk and Francois R. Le Diberder. „SPlot: A Statistical tool to unfold data distributions“.](https://arxiv.org/abs/physics/0402083)
  - 2012 [D. Martschei, M. Feindt, S. Honc, and J. Wagner-Kuhr. „Advanced event reweighting using multivariate analysis“.](http://iopscience.iop.org/article/10.1088/1742-6596/368/1/012028)


## On Machine Learning Tools and Frameworks

[**FastBDT**](https://github.com/thomaskeck/FastBDT)
[Thomas Keck. „FastBDT: A Speed-Optimized Multivariate Classification Algorithm for the Belle II Experiment“.](https://link.springer.com/article/10.1007/s41781-017-0002-8)

[**TMVA**](http://tmva.sourceforge.net/)
[Andreas Hoecker et al. „TMVA: Toolkit for Multivariate Data Analysis“.](https://arxiv.org/abs/physics/0703039)

[**FANN**](http://fann.sourceforge.net/)
[S. Nissen. Implementation of a Fast Artificial Neural Network Library (fann).](http://fann.sourceforge.net/fann.pdf)

[**SKLearn**](http://scikit-learn.org/)
[F. Pedregosa et al. „Scikit-learn: Machine Learning in Python“.](http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf)

[**hep_ml**](https://arogozhnikov.github.io/hep_ml/)

[**XGBoost**](https://xgboost.readthedocs.io/en/latest/)
[Tianqi Chen and Carlos Guestrin. „XGBoost: A Scalable Tree Boosting System“.](https://arxiv.org/abs/1603.02754)

[**Tensorflow**](https://www.tensorflow.org/)
[Martin Abadi et al. „TensorFlow: A system for large-scale machine learning“](https://arxiv.org/abs/1605.08695)

[**Theano**](http://deeplearning.net/software/theano/)
[Rami Al-Rfou et al. „Theano: A Python framework for fast computation of mathematical expressions“](https://arxiv.org/abs/1605.02688)

[**NeuroBayes**](https://github.com/blue-yonder/NeurobayesCppInterface)
[M. Feindt and U. Kerzel. „The NeuroBayes neural network package“](http://www-ekp.physik.uni-karlsruhe.de/~feindt/acat05-neurobayes)


## On Hardware
  - 16.04.2017 [N. P. Jouppi et. al. „In-Datacenter Performance Analysis of a Tensor Processing Unit“](https://arxiv.org/abs/1704.04760)

## Others
  - Joseph Conrad  „The Secret Sharer“
