{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (advanced)\n",
    "\n",
    "In this exercise you explore the most commonly used machine learning algorithms:\n",
    " - Linear Discriminant Analysis (also known as Fisher's Discriminant)\n",
    " - Decision Trees\n",
    " - Random Forests\n",
    " - Artificial Neural Networks\n",
    " \n",
    "Execute the following cells by pressing \"Shift + Enter\", interpret the results and answer the questions.\n",
    "\n",
    "**This is the advanced version of the excercises. You will require knowledge of Python and Numpy to solve the questions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SciPy Stack\n",
    "\n",
    "The SciPy stack is a\n",
    "**Python-based ecosystem of open-source software for mathematics, science, and engineering**\n",
    "\n",
    "### The SciPy stack includes:\n",
    " * **IPython** Interactive python shell\n",
    " * **NumPy** N-dimensional array package and data format used in SciPy\n",
    " * **Matplotlib** Plotting and data visualization\n",
    " * **SciPy** Scientific computing: Integration, Optimization, Statistics, ...\n",
    " * **Pandas** Data structures & data analysis\n",
    " * **Sympy** Symbolic mathematics\n",
    " \n",
    "Furthermore the SciPy stack includes a package for machine learning called **scikit-learn** (or short sklearn).\n",
    "scikit-learn is a good package to take your first steps in machine learning. Later you probably want to change to a more performant and specialized package like:\n",
    "  * XGBoost (https://github.com/dmlc/xgboost) for Gradient Boosting based on Decision Trees\n",
    "  * Tensorflow (https://www.tensorflow.org/) for Artificial Neural Networks\n",
    "  \n",
    "**We won't use any of these libraries here. Remember this is the advanced exercise. Our goal is to understand the algorithms in detail, hence implement everything ourselves!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "I already prepared two functions which create nice plots like you have seen in the lecture.\n",
    "You should be able to understand the code below, otherwise you probably want to switch to the basic exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, classifier=None, show=True):\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], alpha=0.5, label='Signal')\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], alpha=0.5, label='Background')\n",
    "    plt.legend(loc=1, bbox_to_anchor=(0.3, 1.2))\n",
    "    if classifier is not None:\n",
    "        supportX = np.arange(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 0.05)\n",
    "        supportY = np.arange(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 0.05)\n",
    "        GX, GY = np.meshgrid(supportX, supportY)\n",
    "        GZ = classifier.predict(np.dstack([GX, GY]))\n",
    "        plt.contourf(GX, GY, GZ, alpha=0.5, cmap='Greys', levels=np.linspace(GZ.min(), GZ.max(), 20))\n",
    "        if hasattr(classifier, 'fisher'):\n",
    "            plt.plot([-classifier.fisher[0]*3,classifier.fisher[0]*3],\n",
    "                     [-classifier.fisher[1]*3,classifier.fisher[1]*3], 'k-',lw=4)\n",
    "        if hasattr(classifier, 'root_node'):\n",
    "            def draw(node, xmin, xmax, ymin, ymax):\n",
    "                if 'purity' in node: return\n",
    "                if node['feature'] == 0:\n",
    "                    plt.vlines(x=node['value'], ymin=ymin, ymax=ymax, color='black')\n",
    "                    draw(node['left'], xmin, node['value'], ymin, ymax)\n",
    "                    draw(node['right'], node['value'], xmax, ymin, ymax)\n",
    "                else:\n",
    "                    plt.hlines(y=node['value'], xmin=xmin, xmax=xmax, color='black')\n",
    "                    draw(node['left'], xmin, xmax, ymin, node['value'])\n",
    "                    draw(node['right'], xmin, xmax, node['value'], ymax)\n",
    "            draw(classifier.root_node, supportX.min(), supportX.max(), supportY.min(), supportY.max())\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "def plot_roc(X, y, classifier, show=True):\n",
    "    p = classifier.predict(X)\n",
    "    index = np.argsort(p)\n",
    "    tpr = np.cumsum(y[index]) / y.sum()\n",
    "    fpr = np.cumsum(1.0 - y[index]) / float(len(y) - y.sum())\n",
    "    auc = np.trapz(fpr, tpr)\n",
    "    plt.plot(tpr, fpr, lw=3, label='AUC {:3f}'.format(auc))\n",
    "    plt.legend()\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this first exercise we will work only with a 2d classification task, like you have seen in the lecture.\n",
    "The next cell contains two possible toy datasets:\n",
    "  1. Problem 1: Signal and Background are drawn from a gaussian distribution with different means and covariances\n",
    "  2. Problem 2: Signal is drawn from a gaussian distribution and the background is an open ring around the signal\n",
    "  \n",
    "Here we the more complex problem_2 as default, but you can switch to the simpler problem at any time to debug your code more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "\n",
    "def get_problem_1():\n",
    "    signal = np.random.multivariate_normal([-1.0,-1.0],[[1.0,0.5],[0.5,1.0]],(N,))\n",
    "    bckgrd = np.random.multivariate_normal([1.0,1.0],[[1.0,-0.5],[-0.5,1.0]],(N,))\n",
    "    return signal, bckgrd\n",
    "\n",
    "def get_problem_2():\n",
    "    signal = np.random.multivariate_normal([0.0,0.0],[[2.0,1.0],[1.0,3.0]],(N,))\n",
    "    r, phi = np.random.normal(3.0, size=(N,)), np.random.normal(0.0, 1, size=(N,))\n",
    "    bckgrd = np.c_[r*np.cos(phi), r*np.sin(phi)]\n",
    "    return signal, bckgrd\n",
    "\n",
    "def get_data():\n",
    "    X = np.vstack(get_problem_2())\n",
    "    y = np.hstack([np.ones(N), np.zeros(N)])\n",
    "    return X, y\n",
    "\n",
    "X, y = get_data()\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "The linear discriminant analysis is probably the easiest technique you can imagine.\n",
    "Strictly speaking it is not a machine learning technique, because it justs fits\n",
    "a gaussian distribution to the signal and background.\n",
    "\n",
    "I use the interface of sklearn for all classifiers during the exercise:\n",
    "  1. Hyper-parameters are set in the constructor of the class.\n",
    "  2. The classifier is fitted by passing a training dataset X and the truth y (containing the class labels: 1 for signal and 0 for background) to the **fit** method.\n",
    "  3. The classifier calculates the signal-probability of unlabelled new data using the **predict** method (aside: in contrast to the sklearn classifiers we return a probability instead of the class label)\n",
    "  \n",
    "** Exercise 1 ** Execute the code below\n",
    "\n",
    "** Exercise 2 ** Implement the Quadratic Discriminant Analysis by estimating the covariance of both classes separately and by applying the Neyman Pearson lemma. Is QDA better than LDA for this problem? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDiscriminantAnalysis(object):\n",
    "    def fit(self, X, y):\n",
    "        from numpy.linalg import inv\n",
    "        numerator = np.mean(X[y == 1], axis=0) - np.mean(X[y == 0], axis=0)\n",
    "        denominator = np.cov(X[y == 1].T) + np.cov(X[y == 0].T)\n",
    "        self.fisher = np.dot(inv(denominator), numerator)\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.fisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "plot_data(X, y, lda)\n",
    "plot_roc(X, y, lda, show=False)\n",
    "plot_roc(*get_data(), lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision Trees are usually used with a boosting or bagging technique.\n",
    "But they can also be used on their own.\n",
    "Below you find a simple implementation of a decision tree.\n",
    "\n",
    "** Exercise 1 ** Execute the code below. Understand the implementation\n",
    "\n",
    "** Exercise 2 ** Plot the separation gain of some nodes in the tree by adding the separation_gain array to the node dictionary.\n",
    "\n",
    "** Exercise 3 ** Calculate the feature importances by adding up the separation gains provided by each feature.\n",
    "\n",
    "** Exercise 4 ** Exchange the separation measure \"negative entropy\" with the \"gini-impurity\" (see https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_entropy(s, n):\n",
    "    \"\"\"\n",
    "    Calculate the negative entropy\n",
    "    @param s number of signal events\n",
    "    @param n total number of events\n",
    "    \"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        return np.where((s == 0) | (s==n), 0, s * np.log2(s/n) + (n-s) * np.log2((n-s)/n))\n",
    "\n",
    "def get_separation_gain(sorted_ys):\n",
    "    \"\"\"\n",
    "    Calculate the separation gain for all possible cuts for all features\n",
    "    @param sorted_ys 2d array with the truth values y sorted according to the values x of each feature\n",
    "    \"\"\"\n",
    "    n = len(sorted_ys[0])\n",
    "    s = sorted_ys[0].sum()\n",
    "    sl = np.array([np.cumsum(sorted_y)[:-1] for sorted_y in sorted_ys])\n",
    "    nl = np.array([np.arange(1, n) for sorted_y in sorted_ys])\n",
    "    return get_neg_entropy(sl, nl) + get_neg_entropy(s - sl, n - nl)\n",
    "\n",
    "class DecisionTree(object):\n",
    "    \"\"\"\n",
    "    A simple decision tree\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Create a new decision tree.\n",
    "        The whole tree is saved in form of nested dictionaries\n",
    "        @param max_depth the maximum depth of the tree\n",
    "        \"\"\"\n",
    "        self.root_node = {}\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the tree\n",
    "        @param X 2d array containing the features for each event\n",
    "        @param y 1d array containing the truth label\n",
    "        \"\"\"\n",
    "        self._fit(X, y, 0, self.root_node)\n",
    "        \n",
    "    def _fit(self, X, y, depth, node):\n",
    "        \"\"\"\n",
    "        Fit a sub-tree. We call this function recursively\n",
    "        @param X 2d array containing the features for each event which belongs to this sub-tree\n",
    "        @param y 1d array containing the truth label which belongs to this subtree\n",
    "        \"\"\"\n",
    "        # Check if maximum depth is reached\n",
    "        if self.max_depth is not None and depth == self.max_depth:\n",
    "            return {'purity': y.mean()}\n",
    "        \n",
    "        # Calculate best cut based on the separation gain\n",
    "        index = np.argsort(X.T)\n",
    "        separation_gain = get_separation_gain(y[index])\n",
    "        best_cut = np.unravel_index(np.argmax(separation_gain), separation_gain.shape)\n",
    "        \n",
    "        # Now we fill our dictionary which represents the node of the tree\n",
    "        node['feature'] = best_cut[0]\n",
    "        node['value'] = X[index[best_cut[0], best_cut[1]], best_cut[0]]\n",
    "\n",
    "        # If we have too few events left after the cut we stop growing deeper\n",
    "        if best_cut[1] <= 1 or best_cut[1] >= len(index[best_cut[0]]) - 1:\n",
    "            return {'purity': y.mean()}\n",
    "        \n",
    "        # Now we fit the left and right sub-tree using only the events which pass (or don't pass)\n",
    "        # the best cut we found at this point\n",
    "        left = index[best_cut[0], :best_cut[1]]\n",
    "        right = index[best_cut[0], best_cut[1]:]\n",
    "        node['left'] = self._fit(X[left], y[left], depth+1, {})\n",
    "        node['right'] = self._fit(X[right], y[right], depth+1, {})\n",
    "        return node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the signal-probability for each event\n",
    "        @param X 2d array containing the features of each event\n",
    "        \"\"\"\n",
    "        return np.apply_along_axis(self.predict_single, -1, X)\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Unfortunately there is no way to perform the loop in numpy,\n",
    "        therefore we have to use a slow Python loop here.\n",
    "        \"\"\"\n",
    "        node = self.root_node\n",
    "        while 'purity' not in node:\n",
    "            node = node['left'] if x[node['feature']] < node['value'] else node['right']\n",
    "        return node['purity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(max_depth=10)\n",
    "dt.fit(X, y)\n",
    "plot_data(X, y, dt)\n",
    "plot_roc(X, y, dt, show=False)\n",
    "plot_roc(*get_data(), dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "As explained in the lecture over-fitting can be effectively prevented by averaging\n",
    "many simple methods (so-called weak-learners) to obtain one robust complex model.\n",
    "\n",
    "Here we implement a simple random forest. One advantage of random forest\n",
    "over boosted decision trees is that you can train them easily in parallel.\n",
    "\n",
    "** Exercise 1 ** Execute the code below. Understand the implementation.\n",
    "\n",
    "** Exercise 2 ** Parallelize the code. Which speed-up do you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomForest(object):\n",
    "    \"A simple random forest implementation\"\n",
    "    def __init__(self, ntrees=100, sampling_rate=0.5, max_depth=3):\n",
    "        \"\"\"\n",
    "        Create a new random forest\n",
    "        @param ntrees the number of trees in our forest\n",
    "        @param sampling_rate the fraction of events from the training dataset used for fitting each tree\n",
    "        @param max_depth the maximum depth of each individual tree\n",
    "        \"\"\"\n",
    "        self.ntrees = ntrees\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the forest\n",
    "        @param X 2d array containing the features for each event\n",
    "        @param y 1d array containing the truth label\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        index = np.arange(len(X))\n",
    "        for i in range(self.ntrees):\n",
    "            # Select a random subsample of events (without replacement)\n",
    "            index = random.sample(range(len(X)), int(len(X)*self.sampling_rate))\n",
    "            # Fit a single Decision Tree\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X[index], y[index])\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the signal probability by averaging the response of each tree\n",
    "        @param X 2d array containing the features for each event\n",
    "        \"\"\"\n",
    "        return np.mean(np.array([tree.predict(X) for tree in self.trees]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForest()\n",
    "rf.fit(X, y)\n",
    "plot_data(X, y, rf)\n",
    "plot_roc(X, y, rf, show=False)\n",
    "plot_roc(*get_data(), rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network\n",
    "\n",
    "Artificial neural networks are very hyped at the moment due to a concept called **deep learning**.\n",
    "Here we restrict ourselves to **shallow learning**, that is a neural network with only one hidden layer.\n",
    "\n",
    "** Exercise 1 ** Execute the code below. Understand the implementation.\n",
    "\n",
    "** Exercise 2 ** Replace the activation function \"tanh\" with a relu\n",
    "\n",
    "** Exercise 3 ** Implement **weight-decay** by adding $\\lambda \\sum_{ij} w_{ij}^2$ to the loss function. Hint: You have to calculate the derivative with respect to $w_{ij}$ and add this term to the update rule of the weights.\n",
    "\n",
    "** Exercise 4 ** Implement a momentum term by remembering and adding the previous update to the current one $\\Delta w_t = - \\frac{\\partial \\mathcal{L}}{\\partial w_t} + \\alpha \\Delta w_{t-1}$\n",
    "\n",
    "** Exercise 5 (difficult) ** Implement https://arxiv.org/pdf/1609.01596.pdf and be amazed (you only need to change a few lines of code, but it is very hard to figure out which ones...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A simple neural network implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        \"\"\"\n",
    "        Create a new neural network\n",
    "        @param n_input the number of input neurons\n",
    "        @param n_hidden the number of neurons in the hidden layer\n",
    "        \"\"\"\n",
    "        # We randomly initialize the weights, this is called symmetry breaking\n",
    "        self.w_hi = np.random.normal(size=(n_input+1, n_hidden))\n",
    "        self.w_oh = np.random.normal(size=n_hidden)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the neural network\n",
    "        @param X 2d array containing the features for each event\n",
    "        @param y 1d array containing the truth label\n",
    "        \"\"\"\n",
    "        # Append bias neuron\n",
    "        X = np.apply_along_axis(lambda x: np.append(x, [1]), -1, X)\n",
    "        # Scale y from [0, 1] to [-1, 1]\n",
    "        y = 2 * y - 1\n",
    "        for step in range(1, 1000):\n",
    "            # Propagate input through hidden and output layer\n",
    "            a_h = np.dot(X, self.w_hi)\n",
    "            x_h = np.tanh(a_h)\n",
    "            a_o = np.dot(x_h, self.w_oh)\n",
    "            x_o = np.tanh(a_o)\n",
    "            # Back-Propagate error signal\n",
    "            d = lambda x: 4*np.cosh(x)**2/(np.cosh(2*x) + 1)**2\n",
    "            e_o = d(a_o)*(x_o - y)\n",
    "            e_h = d(a_h)*np.outer(e_o, self.w_oh)\n",
    "            # Update weights: we slowly decrease our learning rate here\n",
    "            self.w_oh -= 0.01/np.log(step+1) * np.dot(e_o.T, x_h)\n",
    "            self.w_hi -= 0.01/np.log(step+1) * np.dot(X.T, e_h)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the signal probability using the neural network\n",
    "        @param X 2d array containing the features for each event\n",
    "        \"\"\"\n",
    "        # Append bias neuron\n",
    "        X = np.apply_along_axis(lambda x: np.append(x, [1]), -1, X)\n",
    "        # Propagate input trough hidden layer\n",
    "        x_h = np.tanh(np.dot(X, self.w_hi))\n",
    "        # Propagate output of hidden layer through output layer\n",
    "        return np.tanh(np.dot(x_h, self.w_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(2, 4)\n",
    "nn.fit(X, y)\n",
    "plot_data(X, y, nn)\n",
    "plot_roc(X, y, nn, show=False)\n",
    "plot_roc(*get_data(), nn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
