{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (basic)\n",
    "\n",
    "In this exercise you explore the most commonly used machine learning algorithms:\n",
    " - Linear Discriminant Analysis (also known as Fisher's Discriminant)\n",
    " - Decision Trees\n",
    " - Boosted Decision Trees\n",
    " - Artificial Neural Networks\n",
    " \n",
    "Execute the following cells by pressing \"Shift + Enter\", interpret the results and answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SciPy Stack\n",
    "\n",
    "The SciPy stack is a\n",
    "**Python-based ecosystem of open-source software for mathematics, science, and engineering**\n",
    "\n",
    "### The SciPy stack includes:\n",
    " * **IPython** Interactive python shell\n",
    " * **NumPy** N-dimensional array package and data format used in SciPy\n",
    " * **Matplotlib** Plotting and data visualization\n",
    " * **SciPy** Scientific computing: Integration, Optimization, Statistics, ...\n",
    " * **Pandas** Data structures & data analysis\n",
    " * **Sympy** Symbolic mathematics\n",
    " \n",
    "Furthermore the SciPy stack includes a package for machine learning called **scikit-learn** (or short sklearn).\n",
    "scikit-learn is a good package to take your first steps in machine learning. Later you probably want to change to a more performant and specialized package like:\n",
    "  * XGBoost (https://github.com/dmlc/xgboost) for Gradient Boosting based on Decision Trees\n",
    "  * Tensorflow (https://www.tensorflow.org/) for Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "I already prepared two functions which create nice plots like you have seen in the lecture.\n",
    "Just execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, classifier=None, show=True):\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], alpha=0.5, label='Signal')\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], alpha=0.5, label='Background')\n",
    "    plt.legend(loc=1, bbox_to_anchor=(0.3, 1.2))\n",
    "    if classifier is not None:\n",
    "        supportX = np.arange(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 0.05)\n",
    "        supportY = np.arange(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 0.05)\n",
    "        GX, GY = np.meshgrid(supportX, supportY)\n",
    "        GZ = np.array([[float(classifier.predict_proba(np.array([[x, y]]))[:, 1]) for x in supportX] for y in supportY])\n",
    "        plt.contourf(GX, GY, GZ, alpha=0.5, cmap='Greys', levels=np.linspace(GZ.min(), GZ.max(), 20))\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "def plot_roc(X, y, classifier, show=True):\n",
    "    p = classifier.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(y, p)\n",
    "    auc = sklearn.metrics.roc_auc_score(y, p)\n",
    "    plt.plot(1-tpr, 1-fpr, lw=3, label='AUC {:3f}'.format(auc))\n",
    "    plt.legend()\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this first exercise we will work only with a 2d classification task, like you have seen in the lecture.\n",
    "The next cell contains two possible toy datasets:\n",
    "  1. Problem 1: Signal and Background are drawn from a gaussian distribution with different means and covariances\n",
    "  2. Problem 2: Signal is drawn from a gaussian distribution and the background is an open ring around the signal\n",
    "  \n",
    "In an exercise further down you will be asked to change the dataset from \"problem_1\" to \"problem_2\".\n",
    "You can do this by exchanging the function call in the get_data() function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "\n",
    "def get_problem_1():\n",
    "    signal = np.random.multivariate_normal([-1.0,-1.0],[[1.0,0.5],[0.5,1.0]],(N,))\n",
    "    bckgrd = np.random.multivariate_normal([1.0,1.0],[[1.0,-0.5],[-0.5,1.0]],(N,))\n",
    "    return signal, bckgrd\n",
    "\n",
    "def get_problem_2():\n",
    "    signal = np.random.multivariate_normal([0.0,0.0],[[2.0,1.0],[1.0,3.0]],(N,))\n",
    "    r, phi = np.random.normal(3.0, size=(N,)), np.random.normal(0.0, 1, size=(N,))\n",
    "    bckgrd = np.c_[r*np.cos(phi), r*np.sin(phi)]\n",
    "    return signal, bckgrd\n",
    "\n",
    "def get_data():\n",
    "    X = np.vstack(get_problem_1())\n",
    "    y = np.hstack([np.ones(N), np.zeros(N)])\n",
    "    return X, y\n",
    "\n",
    "X, y = get_data()\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "The linear discriminant analysis is probably the easiest technique you can imagine.\n",
    "Strictly speaking it is not a machine learning technique, because it justs fits\n",
    "a gaussian distribution to the signal and background.\n",
    "\n",
    "All sklearn classifiers have the same interface:\n",
    "  1. Import the necessary sub-package of sklearn.\n",
    "  2. Create a new instance of the appropriate class (here you can set the hyper-parameters of the method, see below)\n",
    "  3. Fit the instance using a training dataset X and the truth y (containing the class labels: 1 for signal and 0 for background) using the **fit** method.\n",
    "  4. Infer the signal-probability of unlabelled new data using the **predict_proba** method\n",
    "  \n",
    "In the code below the inference part is done by the plot_roc function (see above).\n",
    "We plot the ROC curve twice, once for the training dataset (in blue) and once on independent\n",
    "new data we create by calling get_data() (in orange).\n",
    "\n",
    "**Question 1** Why is this classifier called 'linear'?\n",
    "\n",
    "**Question 2** Does the method under-fit or over-fit?\n",
    "\n",
    "**Question 3** Is this classifier appropriate for our problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.discriminant_analysis \n",
    "lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "plot_data(X, y, lda)\n",
    "plot_roc(X, y, lda, show=False)\n",
    "plot_roc(*get_data(), lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision Trees are usually used with a boosting or bagging technique.\n",
    "But they can also be used on their own.\n",
    "\n",
    "**Question 1** Does the method under-fit or over-fit?\n",
    "\n",
    "**Question 2** Limit the maximum depth of the tree by passing the argument \"max_depth=2\" to DecisionTreeClassifier() call. Which cuts did the tree apply?\n",
    "\n",
    "**Question 3** Can you find a value of \"max_depth\" which yields better results than LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "dt = sklearn.tree.DecisionTreeClassifier()\n",
    "dt.fit(X, y)\n",
    "plot_data(X, y, dt)\n",
    "plot_roc(X, y, dt, show=False)\n",
    "plot_roc(*get_data(), dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Decision Trees\n",
    "\n",
    "As explained in the lecture over-fitting can be effectively prevented by averaging\n",
    "many simple methods (so-called weak-learners) to obtain one robust complex model.\n",
    "\n",
    "The GradientBoostingClassifier of sklearn offers several interesting hyper-parameters:\n",
    "  - n_estimators defines the number of trees (-> boosting)\n",
    "  - subsample defines the fraction of the events used for each tree (-> bagging)\n",
    "  - max_depth defines the maximum depth of the individual tree\n",
    "You can set these hyper-parameters like you did above in the case of the DecisionTree.\n",
    "Just add the parameters e.g. \"subsample=0.5\" to the GradientBoostingClassifier() call\n",
    "\n",
    "**Question 1** Does the method under-fit or over-fit. How does it perform out-of-the-box compared to a single DecisionTree\n",
    "\n",
    "**Question 2** Play around with the above hyper-parameters. What are the optimal values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "rf = sklearn.ensemble.GradientBoostingClassifier()\n",
    "rf.fit(X, y)\n",
    "plot_data(X, y, rf)\n",
    "plot_roc(X, y, rf, show=False)\n",
    "plot_roc(*get_data(), rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "Artificial neural networks are very hyped at the moment due to a concept called **deep learning**.\n",
    "Here we restrict ourselves to **shallow learning**, that is a neural network with only one hidden layer.\n",
    "\n",
    "Again there are many hyper-parameters you can play with:\n",
    "  - hidden_layer_sizes: default is (100,). You can also set (5,5) that is a nn with two hidden layers with 5 neurons each.\n",
    "  - max_iter  the number of iterations\n",
    "  - batch_size the number of mini batches just per step\n",
    "  - activation: the activation function possible values are \"relu\", \"tanh\", \"logistic\", or the \"identity\"\n",
    "  - shuffle: whether to shuffle the samples in each iteration\n",
    "  \n",
    "**Question 1** Does the method under-fit or over-fit?\n",
    "\n",
    "**Question 2** What are the optimal values for the hyper-parameters given above?\n",
    "\n",
    "**Question 3** Activate **early stopping**. To do this have a look into the documentation of the MLPClassifier. You can do so executing \"sklearn.neural_network.MLPClassifier?\" in a new cell (see below). What does it do? Is there a reason not to use early stopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neural_network\n",
    "nn = sklearn.neural_network.MLPClassifier()\n",
    "nn.fit(X, y)\n",
    "plot_data(X, y, nn)\n",
    "plot_roc(X, y, nn, show=False)\n",
    "plot_roc(*get_data(), nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.neural_network.MLPClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now switch to \"problem_2\" and do the exercises again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
