<!doctype html>
<html>
	<head>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="mobile-web-app-capable" content="yes">
    <link rel="manifest" href="manifest.json">
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

		<title>An overview of Deep Learning</title>

		<meta name="description" content="An overview of Deep Learning">
		<meta name="author" content="Thomas Keck">

		<link rel="stylesheet" href="../reveal.js/css/reveal.css">
		<link rel="stylesheet" href="../reveal.mod/css/theme/black.css">
		<link rel="stylesheet" href="../reveal.js/lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? '../reveal.js/css/print/pdf.css' : '../reveal.js/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	</head>
	<body>
		<div class="reveal">
			<div class="slides" style="height: 100%">
        <!-- Overview -->
				<section style="height: 100%">
				<section>
          <h1>Deep Learning: Selected Ideas and Concepts</h1>

					<p>
          Thomas Keck (<a href="mailto:thomas.keck2@kit.edu">thomas.keck2@kit.edu</a>)
					</p>

        </section>
          
        <section style="height: 100%">
          <h3>Autonomous Driving</h3>
          <iframe width="100%" height="80%" type="text/html" data-src="https://www.youtube.com/embed/PNzQ4PNZSzc" frameborder="0"> </iframe>
          <br>
          <a href="https://arxiv.org/abs/1611.08323">T. Pohlen, et. al. (12/2016)</a>
        </section>
        
        <section style="height: 100%">
          <h3>Reinforcement Learning</h3>
          <iframe allowtransparency="false" style="background: #FFFFFF;" width="100%" height="80%" type="text/html" data-src="https://www.youtube.com/embed/hx_bgoTF7bs" frameborder="0"> </iframe>
          <br>
          <a href="https://arxiv.org/abs/1707.02286">N. Heess, et. al. (07/2017)</a>
        </section>
        
        <section style="height: 100%">
          <h3>Speech Recognition & Speech Synthesis</h3>
          <iframe allowtransparency="false" style="background: #FFFFFF;" width="100%" height="80%" type="text/html" data-src="https://www.youtube.com/embed/vmINGWsyWX0" frameborder="0"> </iframe>
        </section>
        
        <section style="height: 100%">
          <h3>Learn More</h3>

          <ul>
            <li><a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">Playing Go</a></li>
            <li><a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">Speech Synthethis</a></li>
          </ul>
        </section>
        
				
        <section style="height: 100%">
          <h2>Reminder: Artificial Neural Networks</h2>
              <div class="fig-container"
                   data-fig-id="neural_network"
                   data-file="animations/neural_network.html" style="width: 100%; height: 100%"></div>
        </section>
				
        <section>
          <h3><div style='color: #9090f0'>History</div></h3>
          <ul  style="font-size: 3vmin">
            <li style='color: #50f050' class="fragment" data-fragment-index="1">1950-1970</li>
            <ul class="fragment" data-fragment-index="1">
              <li>Simple Perceptron without hidden layers</li>
              <li>Assumed incapability to perform operations like exclusive-or (Minsky and Papert 1969)</li>
              <li>Lack of computing power</li>
            </ul>
            <li style='color: #50f050' class="fragment" data-fragment-index="2">1980-2000</li>
            <ul  class="fragment" data-fragment-index="2">
              <li>Invention of Backpropagation $\rightarrow$ Multi Layer Perceptrons</li>
              <li>Assumed incapability to train many layers due to local minima in high-dimensions</li>
              <li>Lack of computing power</li>
              <li>Slowly superseded by methods like SVM and BDTs</li>
            </ul>
            <li style='color: #50f050' class="fragment" data-fragment-index="3">2000-2010</li>
            <ul  class="fragment" data-fragment-index="3">
              <li>Dawn of Deep-Learning ($\approx 10$ layer networks)</li>
              <li>Advances it algorithms (e.g. greedy layer-wise training, ReLU)</li>
              <li>More statistics (big data)</li>
              <li>Massive boost in computing power (due to GPUs)</li>
              <li>Assumed that training even more layers is difficult due to vanishing gradient problem</li>
            </ul>
            <li style='color: #50f050' class="fragment" data-fragment-index="4">2010-2020</li>
            <ul  class="fragment" data-fragment-index="4">
              <li>Deep-Learning (Representation Learning)</li>
              <li>Batch Normalisation and architectures like ResNet allow for 1000 layer networks</li>
              <li>Even more statistics (bigger data)</li>
              <li>Massive boost in computing power (due to dedicated GPUs and TPUs)</li>
              <li>Networks seem to be fundamentally flawed ($\rightarrow$ adversarials)</li>
            </ul>
          </ul>
        </section>
        
        </section>
        
        <section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Deep Neural Networks</div> & <div style='color: #f05050'>Vanishing Gradient Problem</div></h2>
				</section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Example: ResNet</div></h3>
              
          <div style="display: flex; width: 100%;">
          <div style="float: left; width: 50%; height: 80vmin">
            <img src="images/resnet.png">
            <br>
            <a href="https://arxiv.org/abs/1512.03385">K. He, et. al. (12/2015)</a>
          </div>
          <div style="float: right; width: 50%; height: 80vmin">
            <div style="height: 10vmin"></div>
            <div style="color: #50f050">Techniques</div>
            <ul>
              <li>ReLU Activation</li>
              <li>He Initialisation</li>
              <li>Batch Normalisation</li>
              <li>Residual Network</li>
            </ul>
            <div style="height: 5vmin"></div>
            <p align="left">
            34 layers (authors explored up to 1202 layers) used for image classification
            </p>
          </div>
          </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Vanishing Gradient Problem</div></h3>
          
              <div style="height: 2vmin"></div>
                 $$ y_{i+1} = \sigma \left(\sum^{N_i} w_i y_i \right) $$ 
              <div style="height: 2vmin"></div>
              
              <div class="fragment"> 
                Activation function is applied for each layer<br> $\rightarrow$ exploding or vanishing <div style="color: #50f050; display: inline">activation</div> / <div style="color: #9090f0; display: inline">gradient
                </div>
                <br>
                $$ y_n = \sigma \left( \dots \sigma \left( \dots \sigma\left( \sum^{N_0} w_0 x \right) \right) \right) $$
              </div>
              
              <div class="fig-container fragment"
                   data-fig-id="neural_network"
                   data-file="animations/vanishing_gradient_problem.html" style="width: 100%; height: 50%"></div>

              <div class="fragment" style="position: absolute; left:20%; top: 85%">
                Training becomes unstable $\rightarrow$ very slow or no convergence
              </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>ReLU Activation Function</div></h3>

              <div style="height: 30vmin">
              <img data-src="images/activation_functions.png" style="background:none; border:none; box-shadow:none;">
              </div>
              <div style="display: flex; width: 100%;">
              <div style="float: left; width: 50%; height: 15vmin">

                <div class="fragment">
                $$ \frac{\mathrm{d}}{\mathrm{d}x} \tanh = \frac{1}{\cosh^2} \le 1$$
                </div>
             
                <div style="height: 5vmin"></div>
                <div class="fragment" style="color: #50f050">Gradient vanishes in deep networks</div>
              </div>
              <div style="float: right; width: 50%; height: 15vmin">
                <div class="fragment">
                $$ \frac{\mathrm{d}}{\mathrm{d}x} \max(0, x) = \left\lbrace \begin{array}{l} 1 \quad \textrm{for x } \gt 0  \\ 0 \quad \textrm{otherwise} \end{array} \right.$$
                </div>
                <div style="height: 5vmin"></div>
                <div class="fragment" style="color: #50f050">Gradient does neither vanish nor explode</div>
              </div>
              </div> 
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>He Initialization</div></h3>

          <div style="height: 5vmin"></div>
             $$ y_{i+1} = \max \left(0,  \sum^{N_i} w_i y_i \right) $$ 
          <div style="height: 2vmin"></div>
              
              <div class="fragment"> 
                <div style="color: #f05050">
                  Weights can still lead to exploding or vanishing activations/gradients if
                </div>
                $$ \frac{1}{2} N_i \mathrm{Var}(w_i) \neq 1 $$
              </div>

          <div style="height: 5vmin"></div>
              
          <div class="fragment">
            <div style="color: #50f050">He Initialization (for ReLU)</div><br>
              $$ w_i \sim \mathcal{N}\left(0, \frac{2}{N_i} \right) $$ <a href="https://arxiv.org/abs/1502.01852">K. He, et. al. (02/2015)</a>
          </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Batch Normalisation</div></h3>

          <div style="height: 5vmin"></div>
            $$ y_{i+1} = \max \left(0,  \sum^{N_i} w_i y_i \right) \quad \quad  w_i \sim \mathcal{N}\left(0, \frac{2}{N_i} \right)$$ 
          <div style="height: 2vmin"></div>
              
              <div class="fragment"> 
                Weights are adjustable
                <div style="color: #f05050">
                  $\rightarrow$ weights can still lead to exploding or vanishing activations/gradients
                </div>
              </div>

          <div style="height: 5vmin"></div>
              
          <div class="fragment">
            <div style="color: #50f050">Batch Normalisation</div><br>
            $$ \hat{y_i} = \gamma \frac{y_i - \mathrm{E}(y_i)}{\sqrt{\mathrm{Var}(y_i)}} + \beta $$
            <br> 
            <ul style="font-size: 3vmin">
              <li>Normalise inputs of activation function with respect to batch</li>
              <li>Introduce learnable parameters to restore representation power</li>
              <li>Regularizes the network (Dropout can be removed)</li>
            </ul>
            <br>
            <a href="https://arxiv.org/abs/1502.03167">S. Ioffe, C. Szegedy (03/2015)</a>
            </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Residual Network</div></h3>
          
          <div>Idea: Split layer into <div style="color: #50f050; display: inline">identity</div> and <div style="color: #9090f0; display: inline">residual</div></div><br>
          $ H(y_i) = $  <div style="color: #9090f0; display: inline">$ F(y_i) $</div> $+$ <div style="color: #50f050; display: inline">$ y_i $</div>
          <br>
          <div class="fig-container"
               data-fig-id="neural_network"
               data-file="animations/residual.html" style="position: absolute; left: 20%; top: 30%; width: 60%; height: 60%"></div>

          <a href="https://arxiv.org/abs/1512.03385">K. He, et. al. (12/2015)</a>
        </section>
        
        </section>
				
				<section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Convolutional Networks</div> & <div style='color: #f05050'>Image Recognition</div></h2>
          Convolution
          Max Pooling
          Inception
          ResNet
				</section>
        
        
        <section style="height: 100%">
          
          <div style="display: flex; width: 100%;">
          <div style="float: left; width: 50%; height: 27vmin">
            <img src="images/bird1.png">
          </div>
          <div style="float: right; width: 50%; height: 27vmin">
            <img src="images/bird2.png">
          </div>
          </div>
          <br>
          <div style="display: flex; width: 100%;">
          <div style="float: left; width: 50%; height: 27vmin">
            <img src="images/bird3.png">
          </div>
          <div style="float: right; width: 50%; height: 27vmin">
            <img src="images/bird4.png">
          </div>
          </div>
          <br>
          <div style="display: flex; width: 100%;">
          <div style="float: left; width: 50%; height: 27vmin">
            <img src="images/bird5.png">
          </div>
          <div style="float: right; width: 50%; height: 27vmin">
            <img src="images/bird6.png">
          </div>
          </div>

          <div class="fragment" style="position: absolute; left: 10%; top: 10%; width: 80%; height: 80%; background:none; border:none; box-shadow:none;">
            <img src="images/xkcd.png"><br>
            <a href="https://xkcd.com/1425/">XKCD</a>
          </div>

        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>State of the Art: Inception-ResNet-V2</div></h3>
              
          <div style="width: 100%; height: 80vmin">
            <img src="images/inception_resnet.jpeg">
            <br>
            <a href="https://arxiv.org/abs/1602.07261">C. Szegedy, et. al. (08/2016)</a>
          </div>
        </section>
        
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Pixel Representation</div></h3>
              
          <div class="fig-container"
                   data-fig-id="convolution"
                   data-file="animations/pixel_representation.html" style="width: 100%; height: 100%"></div>
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Invariance under Transformations</div></h3>

					<div style="height: 5vmin"></div>


					<p align="left">
             Different strategies to build a classifier which is invariant under given transformations in the input space:
					</p>

					<ul>
						<li class="fragment">Extract hand-crafted features that are invariant</li>
						<li class="fragment">Use transformed copies during the training phase</li>
						<li class="fragment">Penalize change in the output under input transformation → Tangent propagation</li>
            <li class="fragment">Build invariance properties into structure of neural network → <div style="color: #50f050; display: inline">Convolution</div></li>
					</ul>
					
					<div style="height: 3vmin"></div>
          
					<div style="display: flex; width: 100%;">
          <div style="float: left; width: 25%; height: 25vmin">
            <img src="images/bird6.png">
          </div>
          <div style="float: right; width: 23%; height: 25vmin">
            <img src="images/bird2.png">
          </div>
          <div style="float: left; width: 25%; height: 25vmin">
            <img src="images/bird3.png">
          </div>
          <div style="float: right; width: 25%; height: 25vmin">
            <img src="images/bird4.png">
          </div>
          </div>
 
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Convolution</div></h3>
              
					<div style="display: flex; width: 100%;">
          <div style="float: left; width: 45%; height: 80vmin">
          <div class="fig-container"
                   data-fig-id="convolution"
                   data-file="animations/convolution.html" style="width: 80%; height: 100%"></div>
          </div> 
          <div style="float: left; width: 55%; height: 25vmin; text-align: left">
          <div style="height: 5vmin"></div>

          $M(x,y) = \sum_c \sum_{ij} $ <div style="color: #f05050; display: inline"> $ K_c(i,j) $ </div> $ P_c(x+i, y+j) $
          <div style="height: 2vmin"></div>

          <div style="color: #50f050">Description</div>
					<ul>
						<li>Learnable filters (e.g. edge detector) organized in feature maps</li>
						<li>Each filter scans the image and detects a specific pattern</li>
						<li>Convolution refers to the spatial dimensions</li>
						<li>Input and outputs channels are still fully connected</li>
					</ul>

          <div style="height: 2vmin"></div>

          <div style="color: #50f050">Hyper-Parameters</div>
					<ul>
            <li>depth – number of filters (also known as kernels)</li>
            <li>size – dimension of the filter e.g. 3 × 3 or 3 × 3 × 4</li>
            <li>stride – step size while sliding the filter through the input</li>
            <li>padding – behavior of the convolution near the borders</li>
          </ul>

          </div>
          </div>
 
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Pooling</div></h3>

					<div style="display: flex; width: 100%;">
          <div style="float: left; width: 45%; height: 80vmin">
          <div class="fig-container"
                   data-fig-id="convolution"
                   data-file="animations/pooling.html" style="width: 80%; height: 100%"></div>
          </div> 
          <div style="float: left; width: 55%; height: 25vmin; text-align: left">
          <div style="height: 5vmin"></div>

          $M(x,y) = $ <div style="color: #f05050; display: inline"> $ \max_{ij} $ </div> $ P(x+i, y+j) $
          <div style="height: 5vmin"></div>

          <div style="color: #50f050">Description</div>
					<ul>
						<li>Takes inputs from small region in the feature maps</li>
						<li>Reduces resolution and computation in following layers</li>
						<li>Increases insensitivity against small shifts</li>
					</ul>


          <div style="height: 5vmin"></div>

          <div style="color: #50f050">Hyper-Parameters</div>
					<ul>
            <li>depth – number of filters (also known as kernels)</li>
            <li>size – dimension of the filter e.g. 2 × 2 or 2 × 2 × 4</li>
            <li>stride – step size while sliding the filter through the input</li>
            <li>padding – behavior of the pooling near the borders</li>
          </ul>
          </div>
          </div>
    
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Inception</div></h3>
          
          <div style="height: 5vmin"></div>
          <div style='color: #f05050'>Convoluted channels are still fully connected</div>
          <ul>
            <li>For instance: Kernel size 3x3 with 100 input and 200 output channels</li>
            <li>Many parameters (3x3x100x200)</li>
            <li>High computational effort</li>
          </ul>
            <br>$\rightarrow$ Most connections will be useless, can we do this in a sparse way?</br>
          <div style="height: 5vmin"></div>
					
          <div class="fragment" style="display: flex; width: 100%;">
          <div style="float: left; width: 45%; height: 40vmin">
         
            Inception-v4 ResNet-v2 Module A
            <br>
            <img data-src="images/Inception_v4__module_a.png">

          </div> 
          <div style="float: left; width: 55%; height: 40vmin; text-align: left">

          <div style="height: 10vmin"></div>

          <div style="color: #50f050">Inception</div>
					<ul>
            <li>Use small kernels in parallel (3x3, 7x1, 1x7)</li>
            <li>Bottleneck Architecture</li>
            <ul>
						  <li>Reduce number of channels before convolution (1x1 conv)</li>
						  <li>Restore number of channels after convolution (1x1 conv)</li>
					</ul>

          </div>
          </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Global Average Pooling</div></h3>
          
          <div style="height: 5vmin"></div>
					<div style="display: flex; width: 100%;">
          <div style="float: left; width: 30%; height: 80vmin">
            <img data-src="images/inception_global.png">
          </div> 
          <div style="float: left; width: 70%; height: 80vmin; text-align: left">
          <div style="height: 5vmin"></div>
          <div style='color: #f05050'>Fully connected layers at the end are fully connected</div>
          <ul>
            <li>For instance: 1792 Channels with a resolution of 8x8</li>
            <li>Many parameters (8x8x1792)</li>
            <li>High computational effort</li>
          </ul>
          <br>
          $\rightarrow$ Most connections will be useless, can we do this in a sparse way?
          <br>
          <div style="height: 5vmin"></div>

          <div class="fragment">
          <div style="color: #50f050">Global Average Pooling</div>
					<ul>
						<li>Position should be not important in the end</li>
						<li>Take the average activation over the entire image</li>
						<li>Reduces computation in following fully-connected layers</li>
					</ul>
          </div>
          </div>
          </div>
    
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Regularization:</div><div style='color: #9090f0'>Dropout</div></h3>
              <p>
              Idea: Prevent overfitting by randomly dropping neurons during the training
              </p>
              <p style="color: #50f050">
              Prevents co-adaption of neurons: <a href="https://arxiv.org/abs/1207.0580">G. E. Hinton, et. al. (06/2012)</a>
              </p>
              <div class="fig-container"
                   data-fig-id="neural_network"
                   data-file="animations/dropout.html" style="position: absolute; top: 30%; left: 15%; width: 70%; height: 70%"></div>
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Summary: Inception-ResNet-V2</div></h3>
              
          <div style="width: 100%; height: 80vmin">
            <img src="images/inception_resnet.jpeg">
            <br>
            <a href="https://arxiv.org/abs/1602.07261">C. Szegedy, et. al. (08/2016)</a>
          </div>
        </section>
				
				</section>
        
				<section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Recurrent Networks</div> & <div style='color: #f05050'>Sequential Data Processing</div></h2>
          Word Embedding
				</section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Animation Recurrent Network</div></h3>

					Animation Recurrent Network          

        </section>
        
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Backpropagation Through Time</div></h3>
          
					<div style="height: 5vmin"></div>
					<div style="width: 100%; height: 40vmin">
            <img src="images/Unfold_through_time.png" style="background:none; border:none; box-shadow:none;">
            <br>
            <small><a href="https://en.wikipedia.org/wiki/Backpropagation_through_time#/media/File:Unfold_through_time.png">Wikipedia</a></small>
          </div>
					<div style="height: 5vmin"></div>

					<p align="left">Problem: Activation function will be applied iteratively ⇒ value (and gradient) vanishes or explodes</p>

        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Solution: Long Short-Term Memory (LSTM) Cell</div></h3>
          
					<div style="width: 100%; height: 40vmin">
            <img src="images/lstm.png" style="background:none; border:none; box-shadow:none;">
            <br>
            <small><a href="https://commons.wikimedia.org/wiki/File:Long_Short_Term_Memory.png">Wikipedia</a></small>
          </div>

					<div style="height: 10vmin"></div>

					<b>Can remember a value for a long time period</b>
					<ul>
						<li>Input gate decides when to update the stored value</li>
						<li>Output gate decides when to output the stored value</li>
						<li>Forget gate decides when to forget the stored value</li>
					</ul>
              
        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #9090f0'>Example: Character-Level Language Model</div></h3>

					Karpathy

        </section>
				
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Example: Show And Tell</div></h3>

					Karpathy

        </section>
        </section>
				
        <section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Playing Games</div> & <div style='color: #f05050'>Reinforcement Learning</div></h2>
          Problem: Correlation of Samples
          Q-Learning and Experience Replace (Value-Based) Bellman-Gleichung
          Async Advantage Actor Critic (A3C) (Policy-Based)
          PCL?
				</section>

				<section style="height: 100%">
          <h3><div style='color: #9090f0'>Example: Continuos Control Tasks</div></h3>
          <iframe width="80%" height="50%" id="ytplayer" type="text/html" data-src="https://www.youtube.com/embed/rAai4QzcYbs" frameborder="0"> </iframe>
						<br>
            <a href="https://arxiv.org/abs/1801.00690">Y. Tassa, et. al. (01/2018)</a>
        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Reinforcement Learning</div></h3>

					<ul>
						<li>Agent interacts with environment $\Epsilon$</li>
						<li>Each time step $t$: receive state $s_t$ and select action $a_t \in \mathcal{A}$ and get a scalar reward $r_t$</li>
						<li>Goal: Maximize discounted return $R_t = \sum_{k=0}^\infty \gamma^k r_{t+k}$</li>
					</ul>

        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Q-Learning</div></h3>

					<div style="color: #50f050">Action Value Network approximates expected return</div>
					$$Q^\pi(s, a) = \mathrm{E}(R_t)$$
        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Advantage Actor-Critic</div></h3>

					<div style="color: #50f050">Action Value Network approximates expected return</div>
					$$Q^\pi(s, a) = \mathrm{E}(R_t)$$
        </section>
				</section>
				
        <section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Adversarials</div> & <div style='color: #f05050'>???</div></h2>
          Adversarials after Goodfellow
          Learning To Attack
				</section>
				</section>

				<section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Outlook</div> & <div style='color: #f05050'>References</div></h2>
				</section>
        </section>
			</div>
		</div>

		<script src="../reveal.js/lib/js/head.min.js"></script>
		<script src="../reveal.js/js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
        controlsTutorial: false,
        center: false,
        transition: 'fade',
        history: true,
        width: '80%',
        height: '100%',
				dependencies: [
          { src: '../reveal.mod/js/d3.v4.min.js' },
          { src: '../reveal.mod/plugin/reveal.js-d3js-plugin/d3js.js' },
					{ src: '../reveal.js/plugin/markdown/marked.js' },
					{ src: '../reveal.js/plugin/markdown/markdown.js' },
					{ src: '../reveal.js/plugin/notes/notes.js', async: true },
					{ src: '../reveal.js/plugin/math/math.js', async: true },
					// { src: '../reveal.js/plugin/highlight/highlight.js', async: false, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
