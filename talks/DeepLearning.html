<!doctype html>
<html>
	<head>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="mobile-web-app-capable" content="yes">
    <link rel="manifest" href="manifest.json">
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

		<title>Machine Learning at Belle II</title>

		<meta name="description" content="An overview of Deep Learning">
		<meta name="author" content="Thomas Keck">

		<link rel="stylesheet" href="../reveal.js/css/reveal.css">
		<link rel="stylesheet" href="../reveal.mod/css/theme/black.css">
		<link rel="stylesheet" href="../reveal.js/lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? '../reveal.js/css/print/pdf.css' : '../reveal.js/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	</head>
	<body>
		<div class="reveal">
			<div class="slides" style="height: 100%">
        <!-- Overview -->
				<section style="height: 100%">
				<section>
          <h1>Deep Learning: Selected Ideas and Concepts</h1>

					<p>
          Thomas Keck (<a href="mailto:thomas.keck2@kit.edu">thomas.keck2@kit.edu</a>)
					</p>
        </section>
				
        <section style="height: 100%">
          <h2>Reminder: Artificial Neural Networks</h2>
              <div class="fig-container"
                   data-fig-id="neural_network"
                   data-file="animations/neural_network.html" style="width: 100%; height: 100%"></div>
        </section>
				
        <section>
          <h3><div style='color: #9090f0'>History</div></h3>
          <ul  style="font-size: 3vmin">
            <li style='color: #50f050' class="fragment" data-fragment-index="1">1950-1970</li>
            <ul class="fragment" data-fragment-index="1">
              <li>Simple Perceptron without hidden layers</li>
              <li>Assumed incapability to perform operations like exclusive-or (Minsky and Papert 1969)</li>
              <li>Lack of computing power</li>
            </ul>
            <li style='color: #50f050' class="fragment" data-fragment-index="2">1980-2000</li>
            <ul  class="fragment" data-fragment-index="2">
              <li>Invention of Backpropagation $\rightarrow$ Multi Layer Perceptrons</li>
              <li>Assumed incapability to train many layers due to local minima in high-dimensions</li>
              <li>Lack of computing power</li>
              <li>Slowly superseded by methods like SVM and BDTs</li>
            </ul>
            <li style='color: #50f050' class="fragment" data-fragment-index="3">2000-2010</li>
            <ul  class="fragment" data-fragment-index="3">
              <li>Dawn of Deep-Learning ($\approx 10$ layer networks)</li>
              <li>Advances it algorithms (e.g. greedy layer-wise training, ReLU)</li>
              <li>More statistics (big data)</li>
              <li>Massive boost in computing power (due to GPUs)</li>
              <li>Assumed that training even more layers is difficult due to vanishing gradient problem</li>
            </ul>
            <li style='color: #50f050' class="fragment" data-fragment-index="4">2010-2020</li>
            <ul  class="fragment" data-fragment-index="4">
              <li>Deep-Learning (Representation Learning)</li>
              <li>Batch Normalisation and architectures like ResNet allow for 1000 layer networks</li>
              <li>Even more statistics (bigger data)</li>
              <li>Massive boost in computing power (due to dedicated GPUs and TPUs)</li>
              <li>Networks seem to be fundamentally flawed ($\rightarrow$ adversarials)</li>
            </ul>
          </ul>
        </section>
        
				
        <section  style="height: 100%">
          <h3><div style='color: #f05050'>Reminder</div></h3>
  
          <div style='height: 5vh'></div>
          <div style='color: #50f050'>
          Universal Function Approximator
          </div>
            $$f(\vec{x}) = \sigma_{o} \left(\sum_{h_n}^{H_n} w_{oh_n} \dots \sigma_{h_1} \left( \sum_i^I w_{h_1i} x_i \right) \right)$$
          <div style='height: 3vh'></div>
          <b>→ can approximate any reasonable function $f: \mathrm{R}^\mathrm{I} \rightarrow \mathrm{R}^\mathrm{O}$</b>
          <br>
          <ul>
            <li>number of inputs $I$</li>
            <li>number of layers $n$</li>
            <li>number of neurons in layer $H_i$</li>
            <li>number of outputs $O$</li>
          </ul>
          
          <div style='height: 5vh'></div>
          <div style='color: #50f050'>
          Stochastic Gradient Descent
          </div>
          $$ \Delta w  = - \eta \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}w} $$
          <div style='height: 3vh'></div>
          <b>→ adjust weights iteratively until convergence</b>
				</section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Activation Functions</div></h3>
          <div style='height: 2vh'></div>
          <div style='color: #50f050'>
            $$\sigma (\underbrace{\sum w x}_{a})$$
          </div>
          <div style='height: 2vh'></div>
          <ul style="font-size: 3vmin">
           <li><div style='color: #9090f0; display: inline'>Desirable properties</div>: Nonlinear, Differentiable, Monotonic, Smooth</li>
           <li><div style='color: #9090f0; display: inline'>Examples for the hidden layer</div>:</li>
              <ul>
                <li>sigmoid $\frac{1}{1+e^{-a}}$: <div style='color: #f05050; display: inline'>not zero-centered</div>, <div style='color: #f05050; display: inline'>saturates</div> $\rightarrow$ don't use this</li>
                <li>$\tanh(a)$: <div style='color: #50f050; display: inline'>zero-centered</div>, <div style='color: #f05050; display: inline'>saturates</div> $\rightarrow$ better than sigmoid</li>
                <li>ReLU $\max(0, a)$: <div style='color: #50f050; display: inline'>constant gradient</div>, <div style='color: #50f050; display: inline'>fast</div>, <div style='color: #f05050; display: inline'>can die out</div></li>
                <li>PReLU $\max(0, a) - \beta \max(0, -a)$:  <div style='color: #50f050; display: inline'>constant gradient</div>, <div style='color: #50f050; display: inline'>fast</div>, <div style='color: #f05050; display: inline'>additional parameter</div> </li>
            </ul>
          </ul>
          <div class="fig-container"
               data-fig-id="neural_network"
               data-file="animations/activation_functions.html" style="width: 100%; height: 100%"></div>
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Loss functions</div></h3>
  
          <div style='height: 2vh'></div>
          <div style='color: #50f050'>
          $$ \Delta w  = - \eta \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}w} $$
          </div>
          <div style='height: 2vh'></div>
          <ul style="font-size: 3vmin">
           <li><div style='color: #9090f0; display: inline'>Desirable properties</div>: Differentiable</li>
           <li><div style='color: #9090f0; display: inline'>Examples</div>:</li>
              <ul>
                <li>Regression: Square Error $\left((f(\vec{x}) - y\right)^2$</li>
                <li>Regression: Absolute Error $\left|(f(\vec{x}) - y\right|$</li>
                <li>Classification: Hinge $\max \left(0, - y \cdot f(\vec{x}) \right)$ (with $y = \pm 1$)</li>
                <li>Classification: Cross-Entropy $-y \ln (f(\vec{x})) - (1-y) \ln (1 - f(\vec{x}))$ (with $y = 0, 1$)</li>
              </ul>
          </ul>
          <div class="fig-container"
               data-fig-id="neural_network"
               data-file="animations/loss_functions.html" style="width: 100%; height: 100%"></div>

				</section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Stochastic Gradient Descent</div></h3>

          <ol style="font-size: 3vmin">
            <li>Feed $N$ samples to the network ($N \hat{=} $ batch-size $\rightarrow$ <div style="color: #50f050; display: inline">stochastic</div>)</li>
            <li>Calculate the <div style="color: #50f050; display: inline">gradient</div> of the average loss with respect to each weight using the chain-rule of analysis</li>
            <li>Adjust the weights in the opposite direction (<div style="color: #50f050; display: inline">descent</div>) with a small step-size (learning-rate) $\eta$</li>
            <li>Repeat until convergence</li>
          </ol>
          
          <div class="fig-container"
               data-fig-id="sgd"
               data-file="animations/stochastic_gradient_descent.html" style="width: 100%; height: 60%"></div>
				</section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Stochastic Gradient Descent</div></h3>

          <ol style="font-size: 3vmin">
            <li>Feed $N$ samples to the network ($N \hat{=} $ batch-size $\rightarrow$ <div style="color: #50f050; display: inline">stochastic</div>)</li>
            <li>Calculate the <div style="color: #50f050; display: inline">gradient</div> of the average loss with respect to each weight using the chain-rule of analysis</li>
            <li>Adjust the weights in the opposite direction (<div style="color: #50f050; display: inline">descent</div>) with a small step-size (learning-rate) $\eta$</li>
            <li>Repeat until convergence</li>
          </ol>
        
          <div style="height: 10vmin"></div>
          <div style="color: #50f050;">
            Many more advanced variants of Stochastic Gradient Descent exists
          </div>
            <ul>
              <li>Older: L-BFGS, AdaGrad, AdaDelta, RMSProb</li>
              <li>ADAM: Adaptive moment estimation <a href="https://arxiv.org/abs/1412.6980">D. P. Kingma, J. Ba (12/2014)</a></li>
              <li>NADAM: ADAM + Nesterov momentum </li>
            </ul>
				</section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Learn-Rate Schedule</div></h3>
          
          <div class="fig-container"
               data-fig-id="sgd"
               data-file="animations/initial_gradient_descent.html" style="width: 100%; height: 100%"></div>
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Learn-Rate vs. Batch-Size vs. Momentum</div></h3>
  
          <div style='height: 2vh'></div>
          $$ \Delta w_t  = - \underbrace{\eta}_{\textrm{Learn-Rate}} \cdot \underbrace{ \frac{\mathrm{d}}{\mathrm{d}w} \frac{1}{N} \sum_i^N \mathcal{L}_i}_{\textrm{Gradient averaged over Batch-Size}} + \underbrace{m \cdot \Delta w_{t-1}}_{\textrm{Momentum term}} $$
          <div style='height: 5vh'></div>
          <p>
          Initial noisy optimization phase is similar to simulated annealing<br> $\rightarrow$ explored region is determined by the noise scale $g$ 
          </p>
          <div style='height: 2vh'></div>
          $$ g \sim \frac{\eta}{N (1-m)} $$

          <a href="https://arxiv.org/abs/1711.00489">S. L. Smith, P. Kindermans, C. Ying, Q. V. Le (02/2018)</a>
          
          <div style='height: 5vh'></div>
          If you increase the Batch-Size you must increase the Learn-Rate!

				</section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Regularization:</div><div style='color: #9090f0'>Early Stopping</div></h3>
              <p>
              Idea: Prevent overfitting by stopping the training before convergence
              </p>
              <p style="color: #50f050">
              Very effective and simple!
              </p>
              <div class="fig-container"
                   data-fig-id="neural_network"
                   data-file="animations/early_stopping.html" style="width: 100%; height: 100%"></div>
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Regularization:</div><div style='color: #9090f0'>$L_1$ and $L_2$ Penalty Terms</div></h3>
              <p>
              Idea: Prevent overfitting by penalize large weights<br>
              Optimal Brain Damage: Forget unimportant stuff<br>
              $$\mathcal{L} \rightarrow \mathcal{L} + L_{\textrm{Penalty}}$$
              </p>
             
              <div style="height: 5vmin"></div>
              <div class="fragment">
              <div style="color: #50f050">
              $$L_2 = \beta \sum_i  w_i^2 $$
              </div>
              <ul>
                <li>Requires careful choice of $\beta$</li>
                <li>Also called Ridge leads to dense representations</li>
                <li>Equivalent to <div style='color: #50f050; display: inline'>weight-decay</div> for Stochastic Gradient Descent</li>
              </ul>
              </div>

              <div style="height: 5vmin"></div>
              <div class="fragment">
              <div style="color: #50f050">
              $$L_1 = \alpha \sum_i | w_i | $$
              </div>
              <ul>
                <li>Requires careful choice of $\alpha$</li>
                <li>Also called LASSO leads to sparse representations</li>
              </ul>
              
              </div>
              
        </section>
        
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Regularization:</div><div style='color: #9090f0'>Dropout</div></h3>
              <p>
              Idea: Prevent overfitting by randomly dropping neurons during the training
              </p>
              <p style="color: #50f050">
              Prevents co-adaption of neurons: <a href="https://arxiv.org/abs/1207.0580">G. E. Hinton, et. al. (06/2012)</a>
              </p>
              <div class="fig-container"
                   data-fig-id="neural_network"
                   data-file="animations/dropout.html" style="position: absolute; top: 30%; left: 15%; width: 70%; height: 70%"></div>
        </section>
        </section>
        
        <section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Deep Neural Networks</div> & <div style='color: #f05050'>Vanishing Gradient Problem</div></h2>
				</section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Example: ResNet</div></h3>
              
          <div style="display: flex; width: 100%;">
          <div style="float: left; width: 50%; height: 80vmin">
            <img src="images/resnet.png">
            <br>
            <a href="https://arxiv.org/abs/1512.03385">K. He, et. al. (12/2015)</a>
          </div>
          <div style="float: right; width: 50%; height: 80vmin">
            <div style="height: 10vmin"></div>
            <div style="color: #50f050">Techniques</div>
            <ul>
              <li>ReLU Activation</li>
              <li>He Initialisation</li>
              <li>Batch Normalisation</li>
              <li>Residual Network</li>
            </ul>
            <div style="height: 5vmin"></div>
            <p align="left">
            34 layers (authors explored up to 1202 layers) used for image classification
            </p>
          </div>
          </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Vanishing Gradient Problem</div></h3>
              
              <div>
                Variance of <div style="color: #f05050; display: inline">activation</div> changes during forward propagation through many layers <br><div style="color: #50f050; display: inline">$\rightarrow$ Activation Vanishes or Explodes</div>
              </div>
              
              <div>
                Variance of <div style="color: #9090f0; display: inline">gradient</div> changes during backward propagation through many layers <br><div style="color: #50f050; display: inline">$\rightarrow$ Gradient Vanishes or Explodes</div>
              </div>

              <div>
              $$ y_{i+1} = \sigma \left( \sum_i w_i y_i \right) $$ 
              </div>
              
              <div class="fig-container"
                   data-fig-id="neural_network"
                   data-file="animations/vanishing_gradient_problem.html" style="width: 100%; height: 50%"></div>

              <div>
                Training becomes unstable $\rightarrow$ very slow or no convergence
              </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>ReLU Activation Function</div></h3>

              <div style="height: 30vmin">
              <img data-src="images/activation_functions.png" style="background:none; border:none; box-shadow:none;">
              </div>
              <div style="display: flex; width: 100%;">
              <div style="float: left; width: 50%; height: 15vmin">
                $$ \frac{\mathrm{d}}{\mathrm{d}x} \tanh = \frac{1}{\cosh^2} \le 1$$
             
                <div style="height: 5vmin"></div>
                <div class="fragment" style="color: #50f050">Gradient vanishes in deep networks</div>
              </div>
              <div style="float: right; width: 50%; height: 15vmin">
                $$ \frac{\mathrm{d}}{\mathrm{d}x} \max(0, x) = \left\lbrace \begin{array}{l} 1 \quad \textrm{for x } \gt 0  \\ 0 \quad \textrm{otherwise} \end{array} \right.$$
                <div style="height: 5vmin"></div>
                <div class="fragment" style="color: #50f050">Gradient does neither vanish nor explode</div>
              </div>
              </div> 
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>He Initialization</div></h3>

          <div style="height: 5vmin"></div>
              $$\mathrm{Var} \left( y_{\mathrm{n}} \right) = \mathrm{Var} \left( y_{1} \right) \left( \prod_{i=2}^n \frac{1}{2} H_i \mathrm{Var} \left( w_i \right) \right)$$
          <div style="height: 2vmin"></div>
              
              <div class="fragment"> 
                <div style="color: #f05050">
                  Weights can still lead to exploding or vanishing activations/gradients if
                </div>
                $$ \frac{1}{2} H_i \mathrm{Var}(w_i) \neq 1 $$
              </div>

          <div style="height: 5vmin"></div>
              
          <div class="fragment">
            <div style="color: #50f050">He Initialization (for ReLU)</div><br>
              $$ w_i \sim \mathcal{N}\left(0, \frac{2}{H_i} \right) $$ <a href="https://arxiv.org/abs/1502.01852">K. He, et. al. (02/2015)</a>
          </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Batch Normalisation</div></h3>

          <div style="height: 5vmin"></div>
              $$\mathrm{Var} \left( y_{\mathrm{n}} \right) = \mathrm{Var} \left( y_{1} \right) \left( \prod_{i=2}^n \frac{1}{2} H_i \mathrm{Var} \left( w_i \right) \right)$$
          <div style="height: 2vmin"></div>
              
              <div class="fragment"> 
                Weights are adjustable
                <div style="color: #f05050">
                  $\rightarrow$ weights can still lead to exploding or vanishing activations/gradients
                </div>
              </div>

          <div style="height: 5vmin"></div>
              
          <div class="fragment">
            <div style="color: #50f050">Batch Normalisation</div><br>
            $$ \hat{y_i} = \gamma \frac{y_i - \mathrm{E}(y_i)}{\sqrt{\mathrm{Var}(y_i)}} + \beta $$
            <br> 
            <ul style="font-size: 3vmin">
              <li>Normalise inputs of activation function with respect to batch</li>
              <li>Introduce learnable parameters to restore representation power</li>
              <li>Regularizes the network (Dropout can be removed)</li>
            </ul>
            <br>
            <a href="https://arxiv.org/abs/1502.03167">S. Ioffe, C. Szegedy (03/2015)</a>
            </div>
              
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Residual Network</div></h3>
          
          <div>Idea: Split <div style="color: #f05050; display: inline">layer</div> into <div style="color: #30a030; display: inline">identity</div> and <div style="color: #5050f0; display: inline">residual</div></div>
          $$ H(y_i) = F(y_i) + y_i $$

          <div class="fig-container"
               data-fig-id="neural_network"
               data-file="animations/residual.html" style="position: absolute; left: 20%; top: 25%; width: 60%; height: 60%"></div>

          <a href="https://arxiv.org/abs/1512.03385">K. He, et. al. (12/2015)</a>
        </section>
        
        </section>
				
				<section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Convolutional Networks</div> & <div style='color: #f05050'>Image Recognition</div></h2>
          Convolution
          Max Pooling
          Inception
          ResNet
				</section>
        
        
        <section style="height: 100%">
          
          <div style="display: flex; width: 100%;">
          <div style="float: left; width: 50%; height: 27vmin">
            <img src="images/bird1.png">
          </div>
          <div style="float: right; width: 50%; height: 27vmin">
            <img src="images/bird2.png">
          </div>
          </div>
          <br>
          <div style="display: flex; width: 100%;">
          <div style="float: left; width: 50%; height: 27vmin">
            <img src="images/bird3.png">
          </div>
          <div style="float: right; width: 50%; height: 27vmin">
            <img src="images/bird4.png">
          </div>
          </div>
          <br>
          <div style="display: flex; width: 100%;">
          <div style="float: left; width: 50%; height: 27vmin">
            <img src="images/bird5.png">
          </div>
          <div style="float: right; width: 50%; height: 27vmin">
            <img src="images/bird6.png">
          </div>
          </div>

          <div class="fragment" style="position: absolute; left: 10%; top: 10%; width: 80%; height: 80%; background:none; border:none; box-shadow:none;">
            <img src="images/xkcd.png"><br>
            <a href="https://xkcd.com/1425/">XKCD</a>
          </div>

        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>State of the Art: Inception-ResNet-V2</div></h3>
              
          <div style="width: 100%; height: 80vmin">
            <img src="images/inception_resnet.jpeg">
            <br>
            <a href="https://arxiv.org/abs/1602.07261">C. Szegedy, et. al. (08/2016)</a>
          </div>
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #f05050'>Invariance under Transformations</div></h3>

					<div style="height: 5vmin"></div>


					<p align="left">
             Different strategies to build a classifier which is invariant under given transformations in the input space:
					</p>

					<ul>
						<li class="fragment">Extract hand-crafted features that are invariant</li>
						<li class="fragment">Use transformed copies during the training phase</li>
						<li class="fragment">Penalize change in the output under input transformation → Tangent propagation</li>
						<li class="fragment">Build invariance properties into structure of neural network → Convolution</li>
					</ul>
					
					<div style="height: 3vmin"></div>
          
					<div style="display: flex; width: 100%;">
          <div style="float: left; width: 25%; height: 25vmin">
            <img src="images/bird6.png">
          </div>
          <div style="float: right; width: 23%; height: 25vmin">
            <img src="images/bird2.png">
          </div>
          <div style="float: left; width: 25%; height: 25vmin">
            <img src="images/bird3.png">
          </div>
          <div style="float: right; width: 25%; height: 25vmin">
            <img src="images/bird4.png">
          </div>
          </div>
 
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Convolution</div></h3>
           
					<ul>
						<li>Learnable filters (e.g. edge detector) organized in feature maps</li>
						<li>Each filter scans the image and detects a specific pattern</li>
					</ul>
 
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Pooling</div></h3>

					<ul>
						<li>Takes inputs from small region in the feature maps</li>
						<li>Reduces resolution and computation in following layers</li>
						<li>Increases insensitivity against small shifts</li>
					</ul>
    
        </section>
        
        <section style="height: 100%">
          <h3><div style='color: #9090f0'>Inception</div></h3>
              
        </section>
				
				</section>
        
				<section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Recurrent Networks</div> & <div style='color: #f05050'>Sequential Data Processing</div></h2>
          Word Embedding
				</section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Animation Recurrent Network</div></h3>

					Animation Recurrent Network          

        </section>
        
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Backpropagation Through Time</div></h3>
          
					<div style="height: 5vmin"></div>
					<div style="width: 100%; height: 40vmin">
            <img src="images/Unfold_through_time.png" style="background:none; border:none; box-shadow:none;">
            <br>
            <small><a href="https://en.wikipedia.org/wiki/Backpropagation_through_time#/media/File:Unfold_through_time.png">Wikipedia</a></small>
          </div>
					<div style="height: 5vmin"></div>

					<p align="left">Problem: Activation function will be applied iteratively ⇒ value (and gradient) vanishes or explodes</p>

        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Solution: Long Short-Term Memory (LSTM) Cell</div></h3>
          
					<div style="width: 100%; height: 40vmin">
            <img src="images/lstm.png" style="background:none; border:none; box-shadow:none;">
            <br>
            <small><a href="https://commons.wikimedia.org/wiki/File:Long_Short_Term_Memory.png">Wikipedia</a></small>
          </div>

					<div style="height: 10vmin"></div>

					<b>Can remember a value for a long time period</b>
					<ul>
						<li>Input gate decides when to update the stored value</li>
						<li>Output gate decides when to output the stored value</li>
						<li>Forget gate decides when to forget the stored value</li>
					</ul>
              
        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #9090f0'>Example: Character-Level Language Model</div></h3>

					Karpathy

        </section>
        </section>
				
        <section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Playing Games</div> & <div style='color: #f05050'>Reinforcement Learning</div></h2>
          Problem: Correlation of Samples
          Q-Learning and Experience Replace (Value-Based) Bellman-Gleichung
          Async Advantage Actor Critic (A3C) (Policy-Based)
          PCL?
				</section>

				<section style="height: 100%">
          <h3><div style='color: #9090f0'>Example: Continuos Control Tasks</div></h3>
						<iframe width="80%" height="50%" id="ytplayer" type="text/html" src="http://www.youtube.com/embed/rAai4QzcYbs?origin=https://thomaskeck.github.io/" frameborder="0" \>
						<br>
            <a href="https://arxiv.org/abs/1801.00690">Y. Tassa, et. al. (01/2018)</a>
        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Reinforcement Learning</div></h3>

					<ul>
						<li>Agent interacts with environment $\Epsilon$</li>
						<li>Each time step $t$: receive state $s_t$ and select action $a_t \in \mathcal{A}$ and get a scalar reward $r_t$</li>
						<li>Goal: Maximize discounted return $R_t = \sum_{k=0}^\infty \gamma^k r_{t+k}$</li>
					</ul>

        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Q-Learning</div></h3>

					<div style="color: #50f050">Action Value Network approximates expected return</div>
					$$Q^\pi(s, a) = \mathrm{E}(R_t)$$
        </section>
				
				<section style="height: 100%">
          <h3><div style='color: #f05050'>Advantage Actor-Critic</div></h3>

					<div style="color: #50f050">Action Value Network approximates expected return</div>
					$$Q^\pi(s, a) = \mathrm{E}(R_t)$$
        </section>
				</section>
				
        <section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Adversarials</div> & <div style='color: #f05050'>???</div></h2>
          Adversarials after Goodfellow
          Learning To Attack
				</section>
				</section>

				<section style="height: 100%">
				<section>
          <h2><div style='color: #9090f0'>Outlook</div> & <div style='color: #f05050'>References</div></h2>
				</section>
        </section>
			</div>
		</div>

		<script src="../reveal.js/lib/js/head.min.js"></script>
		<script src="../reveal.js/js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
        controlsTutorial: false,
        center: false,
        transition: 'fade',
        history: true,
        width: '80%',
        height: '100%',
				dependencies: [
          { src: '../reveal.mod/js/d3.v4.min.js' },
          { src: '../reveal.mod/plugin/reveal.js-d3js-plugin/d3js.js' },
					{ src: '../reveal.js/plugin/markdown/marked.js' },
					{ src: '../reveal.js/plugin/markdown/markdown.js' },
					{ src: '../reveal.js/plugin/notes/notes.js', async: true },
					{ src: '../reveal.js/plugin/math/math.js', async: true },
					// { src: '../reveal.js/plugin/highlight/highlight.js', async: false, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
